{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "\n",
    "### Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* [Fully Connected Layer](#Fully-Connected-Layer)\n",
    "* [Convolutional Layers](#Convolutional-Layer)\n",
    "    * [Convolutional Layers in MXNet](#Convolutional-Layers-in-MXNet)\n",
    "* [Pooling](#Pooling)\n",
    "    * [Pooling in MXNet](#Pooling-in-MXNet)\n",
    "* [RNN and LSTMs](#RNN-and-LSTMs)\n",
    "    * [LSTMS](#LSTMS)\n",
    "    * [LSTMS in MXNet](#LSTMS-in-MXNet)\n",
    "* [Embedding Layers](#Embedding-Layers)\n",
    "    * [Embedding Layers in MXNet](#Embedding-Layers-in-MXNet)   \n",
    "* [Batch Normalization Layers](#Batch-Normalization-Layers)\n",
    "    * [Batch Normalization Layers in MXNet](#Batch-Normalization-Layers-in-MXNet)    \n",
    "* [Dropout Layers](#Dropout-Layers)\n",
    "    * [Dropout Layers in MXNet](#Dropout-Layers-in-MXNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fully connected layer has full connections to all activations in the previous layer. When we defined layers as $W$ in previous chapters, a 'fully connected' layer is what $W$ refers to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "data = mx.sym.Variable('data',shape=[batch_size,100])\n",
    "\n",
    "fc = mx.sym.FullyConnected(data = data, name='fc1', num_hidden=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The CONV layer’s parameters consist of a set of learnable filters based on [the convolutional function](https://en.wikipedia.org/wiki/Convolution). \n",
    "\n",
    "During the forward operation, we slide each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position.\n",
    " \n",
    "<img src='files/conv.png'> \n",
    " \n",
    "This example might look scary but it's not. During the learning, our goal is to learn filters similar to the ones below.  \n",
    " \n",
    "<img src='files/filters.png'>\n",
    "\n",
    "The end result are learned filters that might look like this\n",
    "\n",
    "<img src='files/weights.jpeg'>\n",
    "\n",
    "\n",
    "The goal of convolutions in NNs are to learn filters like these\n",
    "\n",
    "\n",
    "Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. \n",
    "\n",
    "\n",
    "Three important hyperparameters to consider:\n",
    "\n",
    "\n",
    "    1) Depth of the output volume (batch size, height, width, depth) equates directly to the number of filters used in the convolutional layer. Note: each filter is itself a hyperparameter, and in this case, is looking for something separate and distinct within the input.\n",
    "\n",
    "    2) Setting the stride size can dictate the volume size of the output. Setting the stride to 1 moves the filters over one pixel at a time, setting the stride to 2 moves the filters over 2 pixels at a time, etc. Thus, setting the stride to a larger pixel jump results in a smaller volumes spatially. \n",
    "\n",
    "    3) Padding the input is a convenient way to control the size of the output volume. \n",
    "\n",
    "\n",
    "Convolution Arithmetic to ensure that all hyperparameters 'fit':\n",
    "\n",
    "    Number of filters (K)\n",
    "    Input volume size (W)\n",
    "    Kernal size (F)\n",
    "    Stride length (S)\n",
    "    Pad on border (P)\n",
    "    Fit = (W−F+2P)/S+1\n",
    "\n",
    "Together these parameters produces a volume of size W2×H2×D2 where:\n",
    "\n",
    "    W2=(W1−F+2P)/S+1\n",
    "    H2=(H1−F+2P)/S+1 (i.e. width and height are computed equally by symmetry)\n",
    "    D2=K\n",
    "\n",
    "A note on 'fit': if the fit is not an integer, than the hyperparameter choices won't work. \n",
    "\n",
    "Formally: The n-th depth slice (size W2×H2) is the result of performing a valid convolution of the d-th filter over the input volume with a stride of S, and then offset by d-th bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers in MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride Sizes' Effect on Output Shape\n",
      "stride size: (1, 1), result shape: (15L, 200L, 101L, 101L)\n",
      "stride size: (2, 2), result shape: (15L, 200L, 51L, 51L)\n",
      "stride size: (3, 3), result shape: (15L, 200L, 34L, 34L)\n",
      "stride size: (4, 4), result shape: (15L, 200L, 26L, 26L)\n",
      "stride size: (5, 5), result shape: (15L, 200L, 21L, 21L)\n",
      "stride size: (6, 6), result shape: (15L, 200L, 17L, 17L)\n",
      "stride size: (7, 7), result shape: (15L, 200L, 15L, 15L)\n",
      "Kernel Sizes' Effect on Output Shape\n",
      "kernel size: (1, 1), result shape: (15L, 200L, 34L, 34L)\n",
      "kernel size: (2, 2), result shape: (15L, 200L, 34L, 34L)\n",
      "kernel size: (3, 3), result shape: (15L, 200L, 34L, 34L)\n",
      "kernel size: (4, 4), result shape: (15L, 200L, 33L, 33L)\n",
      "kernel size: (5, 5), result shape: (15L, 200L, 33L, 33L)\n",
      "kernel size: (6, 6), result shape: (15L, 200L, 33L, 33L)\n",
      "kernel size: (7, 7), result shape: (15L, 200L, 32L, 32L)\n"
     ]
    }
   ],
   "source": [
    "image_height = 100\n",
    "image_width = 100\n",
    "image_depth = 3 #rgb\n",
    "# 100,100,3\n",
    "\n",
    "\n",
    "reshaped_data_shape = [batch_size, image_depth, image_height, image_width]\n",
    "\n",
    "data = mx.sym.Variable('data',shape = reshaped_data_shape)\n",
    "\n",
    "conv = mx.symbol.Convolution(data=data,\n",
    "                             name='conv',\n",
    "                             kernel=(2, 2), \n",
    "                             pad=(2, 2), \n",
    "                             num_filter=256)\n",
    "\n",
    "print(\"Stride Sizes' Effect on Output Shape\")\n",
    "for size in range(1,8):\n",
    "    conv = mx.symbol.Convolution(data=data,\n",
    "                             name='conv',\n",
    "                             kernel=(2, 2), \n",
    "                             stride=(size,size),\n",
    "                             pad=(1, 1), \n",
    "                             num_filter=200)\n",
    "    op = conv.simple_bind(mx.cpu(), data=reshaped_data_shape)\n",
    "    print(\"stride size: {}, result shape: {}\".format((size,size),op.forward()[0].shape))\n",
    "      \n",
    "print(\"Kernel Sizes' Effect on Output Shape\")\n",
    "for size in range(1,8):\n",
    "    conv = mx.symbol.Convolution(data=data,\n",
    "                             name='conv',\n",
    "                             kernel=(size, size), \n",
    "                             stride=(3,3),\n",
    "                             pad=(1, 1), \n",
    "                             num_filter=200)\n",
    "    op = conv.simple_bind(mx.cpu(), data=reshaped_data_shape)\n",
    "    print(\"kernel size: {}, result shape: {}\".format((size,size),op.forward()[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12aafe750>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX1Ecihq/ojiU/w/Fr0D2QpEqJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c149e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#!http://data.mxnet.io/models/imagenet/resnet/152-layers/resnet-152-symbol.json\n",
    "#!http://data.mxnet.io/models/imagenet/resnet/152-layers/resnet-152-0000.params\n",
    "\n",
    "def norm(x):\n",
    "    ''' for pretty viz '''\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    kernel_0_to_1 = (x - x_min) / (x_max - x_min)\n",
    "    return kernel_0_to_1\n",
    "\n",
    "\n",
    "sym, arg_params, aux_params = mx.model.load_checkpoint('files/resnet-152', 0)\n",
    "\n",
    "conv_layer = norm(arg_params['conv0_weight'].asnumpy())\n",
    "\n",
    "plt.imshow(conv_layer.reshape(56,56,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function of pooling is the attempt at a spatial reduction of the size of a given representation in an effort to reduce the number of parameters and computation in a given network (as well as to control for overfitting)\n",
    "\n",
    "A pooling function accepts a volume of size W1×H1×D1, requiring two hyperparameters:\n",
    "\n",
    "    their spatial extent F\n",
    "    the stride S\n",
    "\n",
    "The output of the function produces a volume of size W2×H2×D2 where:\n",
    "\n",
    "    W2=(W1−F)/S+1\n",
    "    H2=(H1−F)/S+1\n",
    "    D2=D1\n",
    "\n",
    "Introduces zero parameters since it computes a fixed function of the input\n",
    "Note that it is not common to use zero-padding for Pooling layers \n",
    "\n",
    "<img src='files/maxpool.jpeg'>\n",
    "\n",
    "**Getting rid of pooling** Many people dislike the pooling operation and think that we can get away without it because they think it gets rid of valuable representations during the reduction. In attempt to circumvent this, individuals have started using larger strides in CONV layers once in a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling in MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride Kernel Demo\n",
      "kernel: 7\n",
      "stride: 1\n",
      "out shape = (103-7)/(1+1) : (15L, 256L, 97L, 97L)\n",
      "*----*\n",
      "kernel: 6\n",
      "stride: 2\n",
      "out shape = (103-6)/(2+1) : (15L, 256L, 49L, 49L)\n",
      "*----*\n",
      "kernel: 5\n",
      "stride: 3\n",
      "out shape = (103-5)/(3+1) : (15L, 256L, 33L, 33L)\n",
      "*----*\n",
      "kernel: 4\n",
      "stride: 4\n",
      "out shape = (103-4)/(4+1) : (15L, 256L, 25L, 25L)\n",
      "*----*\n",
      "kernel: 3\n",
      "stride: 5\n",
      "out shape = (103-3)/(5+1) : (15L, 256L, 21L, 21L)\n",
      "*----*\n",
      "kernel: 2\n",
      "stride: 6\n",
      "out shape = (103-2)/(6+1) : (15L, 256L, 17L, 17L)\n",
      "*----*\n",
      "kernel: 1\n",
      "stride: 7\n",
      "out shape = (103-1)/(7+1) : (15L, 256L, 15L, 15L)\n",
      "*----*\n"
     ]
    }
   ],
   "source": [
    "reshaped_data_shape = [batch_size, image_depth, image_height, image_width]\n",
    "\n",
    "def print_pool_shape(kernel, stride):\n",
    "    \"\"\"\n",
    "    silly little funct to print the pool operator's shape\n",
    "    \"\"\"\n",
    "    data = mx.sym.Variable('data',shape = reshaped_data_shape)\n",
    "\n",
    "    conv = mx.symbol.Convolution(data=data,\n",
    "                                 name='conv',\n",
    "                                 kernel=(2, 2), \n",
    "                                 pad=(2, 2), \n",
    "                                 num_filter=256)\n",
    "    pool = mx.symbol.Pooling(data=conv, kernel=(kernel, kernel), stride=(stride, stride), pool_type=\"max\")\n",
    "    op = pool.simple_bind(mx.cpu())\n",
    "    print('kernel: {}'.format(kernel))\n",
    "    print('stride: {}'.format(stride))\n",
    "    print(\"out shape = (103-{})/({}+1) : {}\".format(kernel, stride, op.forward()[0].shape))\n",
    "    print('*----*')\n",
    "\n",
    "print(\"Stride Kernel Demo\")\n",
    "shapes = range(1,8)    \n",
    "for x in range(len(shapes)):\n",
    "    print_pool_shape(shapes[::-1][x],shapes[x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrey Karpathy from the cs231 class:\n",
    "\n",
    "    \n",
    "> In practice: use whatever works best on ImageNet. If you’re feeling a bit of a fatigue in thinking about the architectural decisions, you’ll be pleased to know that in 90% or more of applications you should not have to worry about these. I like to summarize this point as “don’t be a hero”: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet  from scratch or design one from scratch. I also made this point at the Deep Learning school.\n",
    "\n",
    "> Compromising based on memory constraints. In some cases (especially early in the ConvNet architectures), the amount of memory can build up very quickly with the rules of thumb  presented above. For example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each and padding 1 would create three activation volumes of size [224x224x64]. This amounts to a total of about 10 million activations, or 72MB of memory (per image, for both activations and gradients). Since GPUs are often bottlenecked by memory, it may be necessary to compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. For example, one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As another example, an AlexNet uses filter sizes of 11x11 and stride of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent nets allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both.\n",
    "\n",
    "The image and the accompanying text is from the blogpost [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). It is one of the best breakdowns on recurrent neural networks and lstms that exist to date (7/1/17). \n",
    "\n",
    "<img src='files/rnndiags.jpeg'>\n",
    "\n",
    "\n",
    ">Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**L**ong **S**hort **T**erm **M**emory)\n",
    "\n",
    "The pictures from this section are taken from the great [Understanding LSTM and its diagrams](https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714) and much of the intuitive explanations are taken from [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "The oringal idea for the LSTM unit was first devised by Sepp Hochreiter and Jürgen Schmidhuber in their 1997 paper 'Long Short-Term Memory'\n",
    "\n",
    "As you can see from the two diagrams below; the original visual image of the LSTM is a bit cumbersome\n",
    "<img src='files/lstm_network_original.png'>\n",
    "\n",
    "<img src='files/lstm_diagram_original.png'>\n",
    "\n",
    "Thankfully, great minds have clarified its structure in order help individuals visually underunderstand what is happening.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/lstm.png'>\n",
    "\n",
    "<img src='files/lstmchain.png'>\n",
    "\n",
    "<img src='files/lstmmem.png'>\n",
    "The input is the old memory (a vector). The first cross ✖ it passes through is the forget valve. It is actually an element-wise multiplication operation. So if you multiply the old memory C_t-1 with a vector that is close to 0, that means you want to forget most of the old memory. You let the old memory goes through, if your forget valve equals 1.  \n",
    "\n",
    "Then the second operation the memory flow will go through is this + operator.  New memory and the old memory will merge by this operation. How much new memory should be added to the old memory is controlled by another valve, the ✖ below the + sign\n",
    "\n",
    "<img src='files/lstmforget.png'>\n",
    "The first one is called the forget valve. It is controlled by a simple one layer neural network. The inputs of the neural network is h_t-1, the output of the previous LSTM block, X_t, the input for the current LSTM block, C_t-1, the memory of the previous block and finally a bias vector b_0. This neural network has a sigmoid function as activation, and it’s output vector is the forget valve, which will applied to the old memory C_t-1 by element-wise multiplication.\n",
    "\n",
    "<img src='files/lstmnewmem.png'>\n",
    "Now the second valve is called the new memory valve. Again, it is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory. The new memory itself, however is generated by another neural network. It is also a one layer network, but uses tanh as the activation function. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory.\n",
    "\n",
    "<img src='files/lstmupdate.png'>\n",
    "These two ✖ signs are the forget valve and the new memory valve.\n",
    "\n",
    "<img src='files/lstmoutput.png'>\n",
    "And finally, we need to generate the output for this LSTM unit. This step has an output valve that is controlled by the new memory, the previous output h_t-1, the input X_t and a bias vector. This valve controls how much new memory should output to the next LSTM unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMS in MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(25000, 'train sequences')\n",
      "(25000, 'test sequences')\n",
      "Pad sequences (samples x time)\n",
      "('x_train shape:', (25000, 80))\n",
      "('x_test shape:', (25000, 80))\n",
      "[DataDesc[data,(32, 80L),<type 'numpy.float32'>,NCHW]]\n",
      "[DataDesc[target,(32, 1L),<type 'numpy.float32'>,NCHW]]\n"
     ]
    }
   ],
   "source": [
    "############ load data ####################\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "train_iter = mx.io.NDArrayIter(data = x_train,\n",
    "                               label = y_train,\n",
    "                               data_name = 'data',\n",
    "                               label_name = 'target',\n",
    "                               batch_size = batch_size,\n",
    "                               shuffle = True)\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "test_iter = mx.io.NDArrayIter(data = x_test,\n",
    "                               label = y_test,\n",
    "                               data_name = 'data',\n",
    "                               label_name = 'target',\n",
    "                               batch_size = batch_size,\n",
    "                               shuffle = False)\n",
    "print(train_iter.provide_data)\n",
    "print(train_iter.provide_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define A Network Capable of using an LSTM \n",
    "\n",
    "#### Create a data and a target symbolic variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(stream = sys.stdout, level = logging.DEBUG)\n",
    "\n",
    "# Define the Network #\n",
    "data = mx.symbol.Variable('data')\n",
    "target = mx.symbol.Variable('target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a data and a target symbolic variable\n",
    "\n",
    "By using the LSTMCell function, we first need to convert sequential information into an embedding so that it can be read by and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = mx.symbol.Embedding(data = data,\n",
    "                                input_dim = max_features, \n",
    "                                output_dim = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have (or plan to have) more than one Recurrent layer within MXNet, you need to use the SequentialRNNCell() function to add layers. This function allows the stacking of multiple layers, essentially this class acts as a list of which appends new layers into a stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack = mx.rnn.SequentialRNNCell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply call 'add' to the stack to pack an LSTMCell with a W size of 128 and a DropoutCell in order to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack.add(mx.rnn.LSTMCell(num_hidden = 128))\n",
    "stack.add(mx.rnn.DropoutCell(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll call 'unroll()' on the stack. This function unrolls an RNN for a given number of (>=1) time steps, where the length (int) is the number of steps to unroll. We've already defined our maxlen parameter as 80 (the time-steps that we're allowing our lstm to go for). The result of this function are two variables, 'output' and 'state'. Outputs (list of Symbol or Symbol) – Symbol (if merge_outputs is True) or list of Symbols (if merge_outputs is False) corresponding to the output from the RNN from this unrolling. 'states' (nested list of Symbol) – The new state of this RNN (the memory) after this unrolling. The type of this symbol is same as the output of begin_state()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output, state = stack.unroll(length = maxlen,\n",
    "                             inputs = embedding, \n",
    "                             merge_outputs = True, \n",
    "                             layout = 'NTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll connect the output of the lstm to a fully connected layer with 1 output (the binary class 0 or 1), depending on if your training data is one-hot encoded - the layer could have 2 outputs. In order to properly set up the loss function, we'll use the LogisticRegressionOutput class. This applies a logistic function to the input.\n",
    "\n",
    "$$\\frac{1}{1+exp(-x)}$$\n",
    "\n",
    "To refresh your memory, this sigmoid is used to squash the real-valued output of a linear model into the [0,1] range so that it can be interpreted as a probability. It is suitable for binary classification or probability prediction tasks. \n",
    "\n",
    "Lastly as a refresher from our segment on initializers, we'll use a orthogonal init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Train-acc=0.800112\n",
      "INFO:root:Epoch[0] Time cost=69.310\n",
      "INFO:root:Epoch[0] Validation-acc=0.833040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('acc', 0.83304028132992325)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = mx.symbol.FullyConnected(data = output, num_hidden = 1)\n",
    "pred = mx.symbol.LogisticRegressionOutput(data = fc, label = target)\n",
    "\n",
    "# declare the model \n",
    "model = mx.mod.Module(symbol = pred,\n",
    "                      data_names = ['data'],\n",
    "                      label_names = ['target'],\n",
    "                      context = mx.cpu())\n",
    "\n",
    "# the cross-entropy loss function for mxnet evaluation callback\n",
    "metric = mx.metric.CustomMetric(feval=lambda labels, pred: ((pred > 0.5) == labels).mean(),\n",
    "                                name=\"acc\") \n",
    "\n",
    "# fit the model using the metric we've defined above\n",
    "model.fit(train_data = train_iter,\n",
    "          eval_data = test_iter,\n",
    "          optimizer = 'adam',\n",
    "          eval_metric = metric,\n",
    "          num_epoch = 1,\n",
    "          initializer = mx.initializer.Orthogonal(0.25))\n",
    "\n",
    "# output the score defined by the metric\n",
    "model.score(test_iter, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plain and simple, embeddings act as a parameterized function that maps one object into a high dimensional representation. Most commonly, this object is a word or a given word's index mapped into a multidimensional array; eg, $W(cat)=(0.2, -0.4, 0.7, ...)$. Typically, the function is a lookup table, parameterized by a matrix, $\\theta$, with a row for each object (word): $W\\theta(wn)=\\theta n$).\n",
    "\n",
    "These structures are used typically in Natural Language Processing to represent words and in [Computer Vision to map faces onto embedded lookup tables](https://arxiv.org/abs/1503.03832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layers in MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Shape: [(10L, 64L, 16L)]\n"
     ]
    }
   ],
   "source": [
    "# vocab size\n",
    "vocabulary_size = 26\n",
    "seq_len, batch_size = (10, 64)\n",
    "\n",
    "# our embedding dimensions\n",
    "embed_dim = 16\n",
    "\n",
    "letters = mx.sym.Variable('letters')\n",
    "embedding_op = mx.sym.Embedding(data=letters, \n",
    "                                input_dim=vocabulary_size, \n",
    "                                output_dim=embed_dim, \n",
    "                                name='embed')\n",
    "# end result \n",
    "print(\"Embedding Shape: {}\".format(embedding_op.infer_shape(letters=(seq_len, batch_size))[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import test_utils\n",
    "vocab_size, embed_dim = (26, 16)\n",
    "batch_size = 12\n",
    "word_vecs = test_utils.random_arrays((vocab_size, embed_dim))\n",
    "\n",
    "op = mx.sym.Embedding(name='embed', input_dim=vocab_size, output_dim=embed_dim)\n",
    "x = np.random.choice(vocab_size, batch_size)\n",
    "\n",
    "y = test_utils.simple_forward(op, embed_data=x, embed_weight=word_vecs)\n",
    "\n",
    "y_np = word_vecs[x]\n",
    "\n",
    "test_utils.almost_equal(y, y_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Batch Normalization Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch Normalization***. A recently (2015) developed technique by Ioffe and Szegedy allows for another technique to combat vanishing or exploding gradients.\n",
    "\n",
    "$Z = XW$\n",
    "\n",
    "$\\tilde{Z} = Z - \\dfrac{1}{m}\\sum\\limits_{i=1}^m Z_i; $\n",
    "\n",
    "$\\hat{Z} = \\dfrac{\\tilde{Z}}{\\sqrt{\\epsilon + \\dfrac{1}{m}\\sum_{i=1}^m \\tilde{Z_i,:}^2} }$\n",
    "\n",
    "$H = \\max\\{0,\\gamma \\tilde{Z} + \\beta\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Norm Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a 5 layer, purely linear network with layers labeled $a, b, c, d, e$. \n",
    "\n",
    "The value of $a$ will very much determine the the statistics of the activations by the time you get to $d$. \n",
    "\n",
    "If we take the gradient of $a$ we might get a very large or a very small number depending on the values of the other parameters within the network (the gradient w.r.t. $a$ is $bcde$.)\n",
    "\n",
    "By adding normalization steps after each multiplication, the value of $a$ now doesn't actually determine the mean or the standard deviation of $d$. Because of this normalization step, we're able to knockout some of the most important interactions between layers. \n",
    "\n",
    "Thus, rather than looking at $a$,$b$,$c$, and $d$ to deterime the mean and standard deviation of $e$; we only need to look at the $gamma$ and $beta$ parameters that we've introduced.\n",
    "\n",
    "For a great overview of BatchNorm logic, please see [this great lecture by Ian Goodfellow](https://www.youtube.com/embed/Xogn6veSyxA?start=325&end=664&version=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization Layers in MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "def batchnorm_forward_numpy(x, gamma, beta, eps):\n",
    "    N, D = x.shape\n",
    "    #step1: calculate mean\n",
    "    mu = 1./N * np.sum(x, axis = 0)\n",
    "    #step2: subtract mean vector of every trainings example\n",
    "    xmu = x - mu\n",
    "    #step3: following the lower branch - calculation denominator\n",
    "    sq = xmu ** 2\n",
    "    #step4: calculate variance\n",
    "    var = 1./N * np.sum(sq, axis = 0)\n",
    "    #step5: add eps for numerical stability, then sqrt\n",
    "    sqrtvar = np.sqrt(var + eps)\n",
    "    #step6: invert sqrtwar\n",
    "    ivar = 1./sqrtvar\n",
    "    #step7: execute normalization\n",
    "    xhat = xmu * ivar\n",
    "    #step8: Nor the two transformation steps\n",
    "    gammax = gamma * xhat\n",
    "    #step9\n",
    "    out = gammax + beta\n",
    "    #store intermediate\n",
    "    cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "    return out, cache\n",
    "\n",
    "def do_batch_norm_numpy(h):\n",
    "    mu = 1/N*np.sum(h,axis =0) # Size (H,) \n",
    "    sigma2 = 1/N*np.sum((h-mu)**2,axis=0)# Size (H,) \n",
    "    hath = (h-mu)*(sigma2+epsilon)**(-1./2.)\n",
    "    y = gamma*hath+beta \n",
    "    return y\n",
    "\n",
    "# mxnet equivalent\n",
    "data = mx.symbol.Variable('data')\n",
    "weight = mx.sym.Variable(name='fc1_weight')\n",
    "fc1 = mx.symbol.FullyConnected(data = data, \n",
    "                               weight=weight, \n",
    "                               name='fc1', \n",
    "                               num_hidden=128)\n",
    "bn1 = mx.symbol.BatchNorm(fc1, name='bn1')\n",
    "act1 = mx.symbol.Activation(bn1, name='relu1',act_type=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number regularization strategies, but for the sake of this workshop, we'll only talk about the main one used in Neural Networks. The strategy known as **Dropout** is implemented by only keeping a neuron active with some probability p (a hyperparameter - often set by the user to be between .2 and .5), or setting it to zero otherwise. Dropout prevents co-adaptation of neurons by making the presence of other neurons unavailable. In this way, a given neuron cannot rely on other neurons to correct it's mistakes. The diagram below shows the Dropout strategy:\n",
    "\n",
    "<img src='files/dropout.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layers in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Inverted Dropout: Recommended implementation example.\n",
    "We drop and scale at train time and don't do anything at test time.\n",
    "More or less, dropout has been shown to be the first-order equivalent to an L2 regularizer. \n",
    "\"\"\"\n",
    "\n",
    "p = 0.5 # probability of keeping a unit active. higher = less dropout\n",
    "\n",
    "def train_step(X):\n",
    "    # forward pass for example 3-layer neural network\n",
    "    H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "    U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!\n",
    "    H1 *= U1 # drop!\n",
    "    H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "    U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!\n",
    "    H2 *= U2 # drop!\n",
    "    out = np.dot(W3, H2) + b3\n",
    "  \n",
    "    # backward pass: compute gradients... (not shown)\n",
    "    # perform parameter update... (not shown)\n",
    "\n",
    "def predict(X):\n",
    "    # ensembled forward pass\n",
    "    H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary\n",
    "    H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "    out = np.dot(W3, H2) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layers in MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "[[ 3.75   0.625 -0.625  0.     8.75 ]\n",
      " [ 0.    -0.5    0.     3.75   0.25 ]]\n",
      "Testing\n",
      "[[ 3.          0.5        -0.5         2.          7.        ]\n",
      " [ 2.         -0.40000001  7.          3.          0.2       ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krzum/anaconda3/envs/ml/lib/python2.7/site-packages/ipykernel/__main__.py:23: UserWarning: Calling forward the second time after forward(is_train=True) without calling backward first. Is this intended?\n"
     ]
    }
   ],
   "source": [
    "# Dropout in mxnet\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "np.random.seed(998)\n",
    "\n",
    "# create input data\n",
    "input_array = np.array([[3., 0.5,  -0.5,  2., 7.],\n",
    "                    [2., -0.4,   7.,  3., 0.2]])\n",
    "\n",
    "# create symbol and executor\n",
    "a = mx.symbol.Variable('a')\n",
    "dropout = mx.symbol.Dropout(a, p = 0.2)\n",
    "executor = dropout.simple_bind(mx.cpu(0),a = input_array.shape)\n",
    "\n",
    "## If training\n",
    "executor.forward(is_train = True, a = input_array)\n",
    "print(\"Training\")\n",
    "print(executor.outputs[0].asnumpy())\n",
    "#[[ 3.75   0.625 -0.     2.5    8.75 ]\n",
    "# [ 2.5   -0.5    8.75   3.75   0.   ]]\n",
    "\n",
    "## If testing\n",
    "executor.forward(is_train = False, a = input_array)\n",
    "print(\"Testing\")\n",
    "print(executor.outputs[0].asnumpy())\n",
    "#[[ 3.     0.5   -0.5    2.     7.   ]\n",
    "# [ 2.    -0.4    7.     3.     0.2  ]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
