{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations\n",
    "\n",
    "***For a much more in depth overview of the foundations of deep learning, please refer to these free resources:***\n",
    "\n",
    "***http://cs224d.stanford.edu/lecture_notes/LectureNotes3.pdf ***\n",
    "\n",
    "***http://cs231n.github.io/***\n",
    "\n",
    "***http://neuralnetworksanddeeplearning.com***\n",
    "\n",
    "***http://www.deeplearningbook.org/***\n",
    "\n",
    "\n",
    "### Contents\n",
    "* [Neurons](#Neurons)\n",
    "\n",
    "* [A Single Layer of Neurons](#A-Single-Layer-of-Neurons)\n",
    "\n",
    "* [Feed-forward Computation](#Feed-forward-Computation)\n",
    "\n",
    "* [Classification Example](#Classification-Example)\n",
    "\n",
    "* [Objective Functions](#Objective-Functions)\n",
    "\n",
    "    * [Softmax Classifier](#Softmax-Classifier)\n",
    "\n",
    "    * [Cross-entropy loss function](#Cross-entropy-loss-function)\n",
    "\n",
    "* [Gradient Decent and Back Propagation](#Gradient-Decent-and-Back-Propagation)\n",
    "\n",
    "    * [Optimization](#Optimization)\n",
    "    * [Backprop via the Chain Rule](#Backprop-via-the-Chain-Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neuron is a singular computational unit that takes *n* inputs and produces a single output.\n",
    "\n",
    "This unit takes an n-dimensional input vector ***x*** and produces the scalar activation (output) ***a***. This neuron is also associated with an n-dimensional weight vector, ***w***, and a bias scalar, ***b***. These neurons are sometimes referred to as units. \n",
    "\n",
    "What differentiates the outputs of different neurons is their parameters (also referred to as their weights). \n",
    "\n",
    "Below, we have a unit called a ReLU, a *rectified linear unit*. Note: There exist many types of units: tahn, sigmoid, etc. ReLU will be the main unit referred to simply because it's the most popular\n",
    "\n",
    "$$\n",
    "a = max \\left(0, w^T \\cdot x + b \\right) \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = f\\left(\n",
    "\\begin{bmatrix}\n",
    "    w_{1}       & w_{2} & \\dots & w_{n}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x_{1}  \\\\\n",
    "    x_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} + b \\right) = max \\left(0, \\begin{bmatrix}\n",
    "    w_{1}       & w_{2} & \\dots & w_{n}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x_{1}  \\\\\n",
    "    x_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} + b \\right) = max \\left(0,  \\sum_i{w_ix_i + b} \\right)\n",
    "$$\n",
    "\n",
    "<img src='files/neuron.jpeg'>\n",
    "\n",
    "### Notes\n",
    "* Put the introduction of the weight vector with the discussion that it is the parameters.\n",
    "* might want to give the definition of the ReLU here since it is being introduced with other functions that are more commonly known.\n",
    "* While it's clear to us how the equation makes sense in matrix notation, it might be nice to have a graphical view of that similar to the equation I added.\n",
    "* $a$ isn't in the diagram and is therefore confusing in the equation\n",
    "* When putting parenthesis in your equations, they automatically scale and look better if you use \\left( and \\right)\n",
    "* The relationship between the matrix notation and summation notation is a nice place to mention where BLAS and GPU can help speed things up if you want to go down that road.\n",
    "* The above image seems at odds with the image below where we are to assume that the activation function is part of the neuron circle. We should figure out a consistent way to pictorally represent things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08220609]\n"
     ]
    }
   ],
   "source": [
    "# single nueron example\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "# forward-pass of a 3-layer neural network:\n",
    "# activation function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "# define our variables\n",
    "#---------------------\n",
    "#This doesn't fit with the image above. W should be the same dimensionality as x\n",
    "#I chaned it to 3 instead of 1 so that the cell executes.\n",
    "#----------------------\n",
    "W1 = np.random.randn(3)\n",
    "b1 = np.random.randn(1)\n",
    "\n",
    "# random input vector of three numbers (3x1)\n",
    "#---------------------\n",
    "#This was a single random number, so I updated it to be 3\n",
    "#----------------------\n",
    "x = np.random.randn(3)\n",
    "\n",
    "# calculate first hidden layer activations (3x1)\n",
    "#-------------------------\n",
    "# We should be careful about what we can an activation. This is labeled as the output above.\n",
    "#-------------------------\n",
    "out = ReLU( np.dot(W1, x) + b1)\n",
    "\n",
    "print(out) \n",
    "#0.69013533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Single Layer of Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extend the idea above to a layer containing $m$ neurons. In the below image, the subscript on $w$ represents a row in the weight matrix.\n",
    "\n",
    "<img src='files/single_layer.png'>\n",
    "\n",
    "Now the equation to get from the inputs to the outputs gains an extra dimension to account for the $m$ neurons in the layer.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    y_{0}  \\\\\n",
    "    x_{1} \\\\\n",
    "    \\vdots \\\\\n",
    "    y_{m}\n",
    "\\end{bmatrix} = f\\left(\n",
    "\\begin{bmatrix}\n",
    "    w_{0,0}       & w_{0,1} & \\dots & w_{0,n} \\\\\n",
    "    w_{1,0}       & w_{1,1} & \\dots & w_{1,n} \\\\\n",
    "    \\vdots       & \\vdots & \\dots & \\vdots \\\\\n",
    "    w_{m,0}       & w_{m,1} & \\dots & w_{m,n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x_{0}  \\\\\n",
    "    x_{1} \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{n}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "    b_{0}  \\\\\n",
    "    b_{1} \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{m}\n",
    "\\end{bmatrix} \\right) = \n",
    "f\\left(\n",
    "\\begin{bmatrix}\n",
    "    \\Theta_{0}  \\\\\n",
    "    \\Theta_{1} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\Theta_{m}\n",
    "\\end{bmatrix}\\right)\n",
    "$$\n",
    "or more compactly\n",
    "$$\n",
    "y = f\\left(wx + b\\right) = max\\left(0, wx + b\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Layers of Neurons\n",
    "\n",
    "## Note\n",
    "I might suggest we change the notation in the below image to make the notation consistent with the math that you are trying to describe below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extend the idea above to multiple neurons by considering the case where the input ***x*** is fed as an input to multiple such neurons\n",
    "\n",
    "<img src='files/onelayer.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we refer to the different neurons’ weights as $\\{w^{(1)}, w^{(2)}, \\cdots, w^{(m)}\\}$ ,\n",
    "\n",
    "the biases as $\\{b^{(1)}, b^{(2)}, \\cdots, b^{(m)}\\}$,\n",
    "\n",
    "we can say the respective activations are $\\{a^{(1)}, a^{(2)}, \\cdots, a^{(m)}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ a^{(1)} = max(0, w^{(1)T} \\cdot x + b_1) \\\\\n",
    "    a^{(2)} = max(0, w^{(2)T} \\cdot x + b_2) \\\\\n",
    "    \\vdots \\\\\n",
    "    a^{(m)} = max(0, w^{(m)T} \\cdot x + b_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some simple notations that let us define more complex networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b =\n",
    "\\begin{bmatrix}\n",
    " b_1\\\\\n",
    " b_2\\\\\n",
    " \\vdots \\\\\n",
    " b_m\n",
    "\\end{bmatrix} \\in {\\Bbb R^{m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W =\n",
    "\\begin{bmatrix}\n",
    " - & w^{(1)} & -\\\\\n",
    " - & w^{(2)} & -\\\\\n",
    " - & \\vdots  & -\\\\\n",
    " - & w^{(m)} & -\n",
    "\\end{bmatrix}  \\in {\\Bbb R^{mxn}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput of the scaing and biases looks like $$ z = Wx + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activations of the ReLU function can then be written as:\n",
    "\n",
    "\n",
    "$$ReLU(z) =\n",
    "\\begin{bmatrix}\n",
    " max(0, z^{(1)})\\\\\n",
    " max(0, z^{(2)})\\\\\n",
    " \\vdots\\\\\n",
    "max(0, z^{(m)})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " a^{(1)}\\\\\n",
    " a^{(2)}\\\\\n",
    " \\vdots\\\\\n",
    " a^{(m)}\n",
    "\\end{bmatrix} = ReLU(z) = ReLU(Wx+b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can think of these activations as indicators of the presence of some weighted combination of features. We can then use a combination of these activations to perform classification/regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks being organized into layers makes it very simple and efficient to evaluate using matrix vector operations. Here is an example of a full forward pass with three matrix multiplications with the application of an activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.77187778]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "\n",
    "# forward-pass of a 3-layer neural network\n",
    "# activation function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0.0,x)\n",
    "\n",
    "#---------------------\n",
    "#It might be helpful to have an image to go along with the neural net you build here\n",
    "#That will allow people to connect the code to the image\n",
    "#Do you think it would be better to define the input first so the reader knows why we are using\n",
    "#the dimensions given below?\n",
    "#---------------------\n",
    "# define our variables\n",
    "W1 = np.random.randn(3,)\n",
    "b1 = np.zeros(3,)\n",
    "W2 = np.random.randn(3,)\n",
    "b2 = np.zeros(3,)\n",
    "W3 = np.random.randn(3,)\n",
    "b3 = np.zeros(1,)\n",
    "\n",
    "# random input vector of three numbers (3x1)\n",
    "x = np.random.randn(3, 1)\n",
    "\n",
    "# calculate first hidden layer activations (3x1)\n",
    "h1 = ReLU( np.dot(W1, x) + b1)\n",
    "\n",
    "# calculate second hidden layer activations (3x1)\n",
    "h2 = ReLU(np.dot(W2, h1) + b2)\n",
    "\n",
    "# output neuron (1x1)\n",
    "out = np.dot(W3, h2) + b3\n",
    "\n",
    "print(out)\n",
    "# 1.77187778\n",
    "\n",
    "# MXNet 3 layer network equivalent\n",
    "#data = mx.symbol.Variable('data')\n",
    "#fc1  = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=3)\n",
    "#act1 = mx.symbol.Activation(data = fc1, name='relu1', act_type=\"relu\")\n",
    "#fc2  = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 3)\n",
    "#act2 = mx.symbol.Activation(data = fc2, name='relu2', act_type=\"relu\")\n",
    "#fc3  = mx.symbol.FullyConnected(data = act2, name = 'fc2', num_hidden = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. An easy way to view this is through classification\n",
    "\n",
    "The first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class\n",
    "\n",
    "#### Training set\n",
    "\n",
    "Our training set is a dataset of images: $x_i \\in R^{D}$ with labels $y_i$ $(i=1...N\\ and\\ y_i\\in1…K)$. We have $N$ examples (each with a dimensionality $D$) and $K$ distinct categories. $N = 50,000$ images, each with $D = 32 \\text{ pixels } \\times 32 \\text{ pixels } \\times 3 \\text{ color channels } = 3072$, and $K= 10$, since there are 10 distinct classes (dog, cat, car, etc).\n",
    "\n",
    "#### Scoring function\n",
    "We will now define the score function, $f:R^{D}\\rightarrow R^{K}$, that maps the raw image pixels to class scores:\n",
    "\n",
    "$f(x_i,W,b)=Wx_i+b$\n",
    "\n",
    "$x_i$ has all of its pixels flattened out to a single column vector of shape [$D$ x 1]. The matrix $W$ (of size [$K$ x $D$]), and the vector $b$ (of size [$K$ x $1$]) are the parameters (people use the terms weights and parameters interchangeably) of the function.\n",
    "\n",
    "<img src='files/imagemap.jpeg'>\n",
    "\n",
    "<font color=\"red\">\n",
    "It might be nice to mention before the end that we are looking at data that can only be a cat, dog, or ship. This way the image won't confuse people on where these labels come from.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48165915  2.13458226 -0.03466199]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "\n",
    "# define our variables\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.zeros(3,)\n",
    "cat_picture = np.random.randn(4, 1)\n",
    "\n",
    "# calculate scores (3x1)\n",
    "scores = np.dot(W1, cat_picture).T + b1\n",
    "\n",
    "print(scores)\n",
    "# [[-0.48165915  2.13458226 -0.03466199]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code predicted that our fictitious cat picture was most likely a dog. This is not the desired behavior of our classifier. We need a way to let the classifier know when it is performing well and when it is making poor predictions. To achieve this, the idea of correctness must be transformed into a mathematical function. Like most machine learning models, neural networks also use an optimization objective, a measure of error or goodness which we want to minimize or maximize respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Softmax classifier is the binary Logistic Regression <font color='red'> We haven't introduced binary logistic regression yet. This might not be a helpful analogy</font> generalization to multiple classes\n",
    "\n",
    "Easy to interpret the output (normalized class probabilities)\n",
    "\n",
    "$ f_j(z) = \\dfrac{e^{z_j}}{\\sum_ke^{z_k}} $\n",
    "\n",
    "It takes a vector of arbitrary real-valued scores (in z) and squashes it to a vector of values between zero and one that sum to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23639688,  0.09442378,  0.66917934]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------\n",
    "#It would be nice to use the same values as above so that the user can see how the \n",
    "#softmax has altered the previous output.\n",
    "#-----------------\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "# define our softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "# define our variables\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,)\n",
    "cat_picture = np.random.randn(4, 1)\n",
    "\n",
    "# calculate scores (1x3)\n",
    "scores = np.dot(W1, cat_picture).T + b1\n",
    "\n",
    "# calculate softmax\n",
    "softmax(scores)\n",
    "\n",
    "# mxnet 3 layer network equivalent\n",
    "#data = mx.symbol.Variable('data')\n",
    "#fc1  = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=3)\n",
    "#mlp = mx.symbol.SoftmaxOutput(data=fc1, name='softmax')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*also referred to as minimizing the negative log liklihood*\n",
    "\n",
    "$L_i = -log\\Bigg(\\dfrac{e^{f_{yi}}}{\\sum_je^{f_{j}}}\\Bigg)$\n",
    "\n",
    "$f_j$ to mean the $j$-th element of the vector of class scores $f$. The full loss for the dataset is the mean of $L_i$\n",
    "\n",
    "The cross-entropy between a “true” distribution $p$ and an estimated distribution $q$ is defined as\n",
    "\n",
    "$H(p,q) = -\\sum p(x)\\log q(x)$\n",
    "\n",
    "The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities and the “true” distribution (also equivalent to minimizing the KL divergence between the two distributions).\n",
    "\n",
    "In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "<font color=\"red\">There is a lot being thrown at the user here without a rigorous proof. Is this information essential to moving forward?</font>\n",
    "\n",
    "**TODO(krzum) introduce regularization**\n",
    "\n",
    "<img src='files/svmvssoftmax.png'>\n",
    "\n",
    "Softmax classifier provides “probabilities”, rather confidences, for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05784432  2.36316069  1.32312981]\n",
      "[ 0.01544932  0.63116335  0.35338733]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "print np.exp([-2.85,0.86,0.28])\n",
    "print np.exp([-2.85,0.86,0.28]) / np.sum(np.exp([-2.85,0.86,0.28]), axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground-truth data: [0 1 0]\n",
      "softmax probabilities: [[ 0.06154678  0.84221806  0.09623515]]\n",
      "np.log(probs)[[-2.78795772 -0.17171631 -2.34096056]]\n",
      "cross-entropy loss: 0.336424664343\n",
      "()\n",
      "ground-truth data: [1 0 0]\n",
      "softmax probabilities: [[ 0.06154678  0.84221806  0.09623515]]\n",
      "np.log(probs)[[-2.78795772 -0.17171631 -2.34096056]]\n",
      "cross-entropy loss: 4.73568515732\n",
      "()\n",
      "ground-truth data: [0 0 1]\n",
      "softmax probabilities: [[ 0.06154678  0.84221806  0.09623515]]\n",
      "np.log(probs)[[-2.78795772 -0.17171631 -2.34096056]]\n",
      "cross-entropy loss: 4.25102418183\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "\n",
    "# define our softmax function\n",
    "#softmax normalizes all inputs between 0 and 1\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "#cross entropy is a measure of distance between predictions and actual\n",
    "def cross_entropy(pred, truth):\n",
    "    return np.sum(np.nan_to_num(-truth*np.log(pred)-(1-truth)*np.log(1-pred)))\n",
    "\n",
    "# define our variables\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.zeros(3,)\n",
    "cat_picture = np.random.randn(4, 1)\n",
    "truth = np.array([0,1,0]).astype('int32')\n",
    "\n",
    "# calculate scores (1x3)\n",
    "scores = np.dot(W1, cat_picture).T + b1\n",
    "\n",
    "# calculate softmax\n",
    "probs = softmax(scores)\n",
    "\n",
    "# calculate loss\n",
    "loss = cross_entropy(probs,truth)\n",
    "\n",
    "print(\"ground-truth data: {}\".format(truth))\n",
    "print(\"softmax probabilities: {}\".format(probs))\n",
    "print('np.log(probs)' +str(np.log(probs)))\n",
    "print(\"cross-entropy loss: {}\".format(loss))\n",
    "print()\n",
    "#ground-truth data: [0 1 0]\n",
    "#softmax probabilities: [[ 0.06154678  0.84221806  0.09623515]]\n",
    "#cross-entropy loss: 0.3364246643429195\n",
    "\n",
    "truth = np.array([1,0,0]).astype('int32')\n",
    "print(\"ground-truth data: {}\".format(truth))\n",
    "print(\"softmax probabilities: {}\".format(probs))\n",
    "print('np.log(probs)' +str(np.log(probs)))\n",
    "loss = cross_entropy(probs,truth)\n",
    "print(\"cross-entropy loss: {}\".format(loss))\n",
    "print()\n",
    "\n",
    "truth = np.array([0,0,1]).astype('int32')\n",
    "print(\"ground-truth data: {}\".format(truth))\n",
    "print(\"softmax probabilities: {}\".format(probs))\n",
    "print('np.log(probs)' +str(np.log(probs)))\n",
    "loss = cross_entropy(probs,truth)\n",
    "print(\"cross-entropy loss: {}\".format(loss))\n",
    "\n",
    "\n",
    "# mxnet 3 layer network equivalent showing SoftmaxOutput\n",
    "#data = mx.symbol.Variable('data')\n",
    "#fc1  = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=3)\n",
    "#softmax_output = mx.symbol.SoftmaxOutput(data=fc1, name='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent and Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**skip this if you want to**\n",
    "\n",
    "We've defined the network structure and the how to penalize a universal function approximator. Now, we'll show how to make this deep thing learn.\n",
    "\n",
    "Note: Gradient Decent and Back Propagation both require study. This will not be a complete reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss function lets us gauge the quality of $W$, and thus the goal of optimization is to find a set of $W$ that minimizes the loss function. The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step. \n",
    "<font color=\"red\">W hasn't been defined as a set of all possible matrices w, so saying find a set of W could be confusing. I'm assuming here that you mean an element of W instead of a set.</font>\n",
    "\n",
    "When functions take a vector of numbers instead of a single number, these derivatives are called **partial derivatives**, and the gradient is simply the vector of partial derivatives in each dimension. <font color=\"red\">This isn't exactly what it means to be a partial derivative. Partial derivatives are defined as derivatives of a function of multiple variables when all but the variable of interest are held fixed during the differentiation. The held fixed part is a really important part of the definition. I have tried to give a different approach below, but feel free to change it to better fit your approach. Somewhere between both of our approaches is probably best.</font>\n",
    "\n",
    "Recall from calculus class that the derivative describes the slope of a function at a given point. When a function depends on multiple variables, the derivative must take into account all of these variables. To calculate the total derivative (taking into account all the variables) it helps to calculate partial derivatives. Partial derivatives describe how a function changes with respect to a single variable while all other variables are held constant. The partial derivative is denoted by the symbol $\\partial$. Using this notation, a function $f\\left(x,y\\right)$ has a total derivative given by\n",
    "$$\n",
    "\\frac{df}{dx} = \\frac{\\partial f}{\\partial x}\\frac{dx}{dx} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dx}.\n",
    "$$\n",
    "\n",
    "Pretending to multiple both sides of the above equation by $dx$ gives the following. (Note that the math doesn't exactly work the same as fractions in the manner we are suggesting here, but this is an instance where pretending gets to the correct answer faster. For a more complete description refer to your preferred calculus text.)\n",
    "$$\n",
    "df = \\frac{\\partial f}{\\partial x}dx + \\frac{\\partial f}{\\partial y}dy.\n",
    "$$\n",
    "\n",
    "The equation above can be separated into the vector part describing the local slope of the function and a vector representing the infinitesimal change in each of the variables.\n",
    "$$\n",
    "df = \\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial x}      & \\frac{\\partial f}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    dx \\\\\n",
    "    dy\\\\\n",
    "\\end{bmatrix} = \\nabla \\begin{bmatrix}\n",
    "    dx \\\\\n",
    "    dy\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The slope portion of the above equation is so frequently used that it is called the gradient, and it represents the slope of a function with respect to independent changes in the function's variables. \n",
    "\n",
    "Take this super basic function for example:\n",
    "\n",
    "$f\\left(x,y\\right) = x y$\n",
    "\n",
    "Calculus allows us to compute the partial derivative for either input:\n",
    "\n",
    "$f(x,y) = x y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x$\n",
    "\n",
    "Derivatives indicate the rate of change of a function with respect to that variable surrounding an infinitesimally small region near a particular point:\n",
    "\n",
    "$\\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop via the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating our $W$ with the Chain Rule (Backprop): When you start to have complicated functions like $f(x,y,z)=(x+y)z$, you can break them down into separate functions, $q=x+y$ and $f=qz$.\n",
    "\n",
    "Because we're interested in $f$ with respect to inputs $x, y, z$, the **chain rule** tells us the correct way to chain these gradient expressions together is with multiplication: $\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}$\n",
    "\n",
    "**Set some inputs**\n",
    "\n",
    "$x = -2; \\\\\n",
    "y = 5; \\\\\n",
    "z = -4$\n",
    "\n",
    "**Perform the forward pass**\n",
    "\n",
    "$ q = x + y$\n",
    "\n",
    "$f = q * z $\n",
    "\n",
    "*q becomes 3 and f becomes -12*\n",
    "\n",
    "**Perform the backward pass (backpropagation) in reverse order:**\n",
    "\n",
    "**first backprop through $f = q * z$**\n",
    "\n",
    "$\\frac{\\partial f}{\\partial z} = q$\n",
    "\n",
    "*gradient on z becomes 3*\n",
    "\n",
    "$\\frac{\\partial f}{\\partial q} = z$\n",
    "\n",
    "*gradient on q becomes -4*\n",
    "\n",
    "**now backprop through $q = x + y$**\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 1*\\frac{\\partial f}{\\partial q}$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = 1*\\frac{\\partial f}{\\partial q}$\n",
    "\n",
    "And boom, the chain-rule\n",
    "\n",
    "***The loss function gives the ability to evaluate $W$, the gradient tells us the sensativity of each variable with respect to the input, and the chain rule gives us the mechanism by which to update weights. Together, these three pieces form the most important parts of Deep Learning.***\n",
    "\n",
    "\n",
    "It should be noted that choosing the step size (also called the learning rate) will become one of the most important (and most headache-inducing) hyperparameter settings in training a neural network\n",
    "<font color=\"red\">I think this section needs something to give a conceptual idea of backpropogation before diving into the math. Something that tells the reader that we need to walk backwards through multiple different layers using the previous layer as the goal/new loss function. Maybe an image would be helpful?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at time step 0: 26444892.357331783\n",
      "Loss at time step 50: 15316.737116928942\n",
      "Loss at time step 100: 695.3155233484949\n",
      "Loss at time step 150: 59.122797880043564\n",
      "Loss at time step 200: 6.4216502602055945\n",
      "Loss at time step 250: 0.7720914698610868\n",
      "Loss at time step 300: 0.09720175415918952\n",
      "Loss at time step 350: 0.012525263136712593\n",
      "Loss at time step 400: 0.0016354783492967572\n",
      "Loss at time step 450: 0.00021537538297857064\n",
      "Loss at time step 500: 2.8532985847947457e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "# http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0.0,x)\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(501):\n",
    "    \n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = ReLU(h)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 50 == 0:\n",
    "        print(\"Loss at time step {}: {}\".format(t, loss))\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    \n",
    "    # compute grad of w2 so you can compute grad of w1\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    \n",
    "    # compute grad of w1\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "## mxnet example\n",
    "#import mxnet as mx\n",
    "#data = mx.symbol.Variable('data')\n",
    "#fc1  = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=512)\n",
    "#act1 = mx.symbol.Activation(data = fc1, name='relu1', act_type=\"relu\")\n",
    "#fc2  = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 512)\n",
    "#act2 = mx.symbol.Activation(data = fc2, name='relu2', act_type=\"relu\")\n",
    "#fc3  = mx.symbol.FullyConnected(data = act2, name='fc3', num_hidden=10)\n",
    "#mlp = mx.symbol.SoftmaxOutput(data=fc3, name='softmax')\n",
    "\n",
    "# Here we instatiate the model for our data\n",
    "#model = mx.mod.Module(\n",
    "#    mlp,                             # Use the network we just defined\n",
    "#    context = mx.cpu(0),             # Run on CPU 0\n",
    "#    data_names = ['data'],           # Provide the name of 'data'\n",
    "#    label_names = ['softmax_label'], # Provide the name of 'label\n",
    "#    )\n",
    "# Fit the model \n",
    "#model.fit(\n",
    "#    train_data = training_data,\n",
    "#    num_epoch = 10,\n",
    "#    optimizer_params = {'learning_rate':0.1},\n",
    "#    batch_end_callback = mx.callback.log_train_metric(500),\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
